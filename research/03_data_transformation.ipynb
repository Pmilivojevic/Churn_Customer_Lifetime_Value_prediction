{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/Churn_Customer_Lifetime_Value_prediction/research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cb03386d-9344-47b1-82f9-868fbb64b4ae/python_projects/Churn_Customer_Lifetime_Value_prediction'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_file: Path\n",
    "    train_file: Path\n",
    "    test_file: Path\n",
    "    transfrmation_params: dict\n",
    "    dataset_val_status: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn_pred.constants import *\n",
    "from churn_pred.utils.main_utils import create_directories, read_yaml\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_file_path = CONFIG_FILE_PATH,\n",
    "        params_file_path = PARAMS_FILE_PATH,\n",
    "        schema_file_path = SCHEMA_FILE_PATH\n",
    "    ):\n",
    "        self.config = read_yaml(config_file_path)\n",
    "        self.params = read_yaml(params_file_path)\n",
    "        self.schema = read_yaml(schema_file_path)\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        transformation_params = self.params.data_transformation\n",
    "        dataset_val_status_file = self.config.data_validation.STATUS_FILE\n",
    "        \n",
    "        with open(dataset_val_status_file, 'r') as f:\n",
    "            status = f.read()\n",
    "        \n",
    "        status = bool(str.split(status)[-1])\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            data_file=Path(config.data_file),\n",
    "            train_file=Path(config.train_file,),\n",
    "            test_file=Path(config.test_file),\n",
    "            transfrmation_params=transformation_params,\n",
    "            dataset_val_status=status\n",
    "        )\n",
    "        \n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def loading_data(self):\n",
    "        df = pd.read_excel(self.config.data_file)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def handling_missing_values(self, df):\n",
    "        print(\"Features names:\",df.columns.to_list())\n",
    "        \n",
    "        null = df.isnull().sum()\n",
    "        null = null[null > 0]\n",
    "        print(\"Names of features of missing values:\",null.index.to_list())\n",
    "        \n",
    "        df['ftd_date'] = df['ftd_date'].fillna(df['reg_date'])\n",
    "        df['qp_date'] = df['qp_date'].fillna(df['ftd_date'])\n",
    "        df['total_handle'] = df['total_handle'].fillna(0.0)\n",
    "\n",
    "        print(df.isnull().sum())\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def handling_datetime_features(self, df):\n",
    "        print(\"Features names:\",df.columns.to_list())\n",
    "        \n",
    "        datetime_features = df.select_dtypes(include='datetime').columns.to_list()\n",
    "        \n",
    "        print(\"Names of datetime features:\", datetime_features)\n",
    "        \n",
    "        df[datetime_features] = df[datetime_features].apply(pd.to_datetime)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def log_transform(self, df):\n",
    "        print(\"Features names:\",df.columns.to_list())\n",
    "        \n",
    "        financial_features = ['total_deposit', 'total_handle', 'total_ngr']\n",
    "        \n",
    "        print(\"Names of financial features:\", financial_features)\n",
    "        \n",
    "        for col in financial_features:\n",
    "            df[f'log_{col}'] = np.log1p(df[col])\n",
    "        \n",
    "        df = df.drop(columns=financial_features)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    def feature_engineering(self, df):\n",
    "        # df = df.sort_values('activity_month')\n",
    "        \n",
    "        df = df.sort_values(['account_id', 'activity_month'])\n",
    "        \n",
    "        df['months_active'] = ((\n",
    "            df['activity_month'].dt.year - df['ftd_date'].dt.year\n",
    "        ) * 12 + (\n",
    "            df['activity_month'].dt.month - df['ftd_date'].dt.month\n",
    "        )).astype(int)\n",
    "        \n",
    "        df['has_qp'] = df['qp_date'].notnull().astype(int)\n",
    "        df['days_ftd_to_qp'] = (df['qp_date'] - df['ftd_date']).dt.days.fillna(-1)\n",
    "        df['reg_date'] = df['reg_date'].dt.month\n",
    "\n",
    "        df['next_activity_month'] = df.groupby('account_id')['activity_month'].shift(-1)\n",
    "        df['months_to_next_activity'] = (df['next_activity_month'] - df['activity_month']).dt.days / 30\n",
    "\n",
    "        # Define churn: no activity in next 2 months = churn\n",
    "        df['churn_next_month'] = (\n",
    "            df['months_to_next_activity'] >= self.config.transfrmation_params.no_activity_thr\n",
    "        ).astype(int)\n",
    "        \n",
    "        # Find player's last activity month\n",
    "        last_month_df = df.groupby('account_id')['activity_month'].max().reset_index()\n",
    "        last_month_df.rename(columns={'activity_month': 'last_activity_month'}, inplace=True)\n",
    "\n",
    "        # Merge back\n",
    "        df = df.merge(last_month_df, on='account_id')\n",
    "\n",
    "        # Compute months since last activity per record\n",
    "        df['months_since_last_activity'] = ((df['last_activity_month'] - df['activity_month']).dt.days) / 30\n",
    "\n",
    "        # Label churners: no activity â‰¥ 2 months after last activity\n",
    "        df['churned'] = (\n",
    "            df['months_since_last_activity'] >= self.config.transfrmation_params.churn_months_thr\n",
    "        ).astype(int)\n",
    "        \n",
    "        # \n",
    "        df['early_churn'] = (\n",
    "            df['months_active'] <= self.config.transfrmation_params.early_churn_thr\n",
    "        ).astype(int)\n",
    "        \n",
    "        df = df.drop(columns=[\n",
    "            'activity_month',\n",
    "            'next_activity_month',\n",
    "            'months_to_next_activity',\n",
    "            'last_activity_month',\n",
    "            'months_since_last_activity',\n",
    "            'ftd_date',\n",
    "            'churned',\n",
    "            'account_id',\n",
    "            'tracker_id',\n",
    "            'qp_date'\n",
    "        ], errors='ignore')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def handling_categorical_features(self, df):\n",
    "        print(\"Features names:\",df.columns.to_list())\n",
    "        \n",
    "        categorical_features = df.select_dtypes(include='object').columns.to_list()\n",
    "        \n",
    "        print(\"Names of categorical features:\", categorical_features)\n",
    "        \n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        encoded = pd.DataFrame(\n",
    "            encoder.fit_transform(df[categorical_features]),\n",
    "            columns=encoder.get_feature_names_out(categorical_features),\n",
    "            index=df.index\n",
    "        )\n",
    "        \n",
    "        df = pd.concat([df, encoded], axis=1)\n",
    "        df.drop(categorical_features, axis=1, inplace=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def train_test_split_and_save(self, df):\n",
    "        split_idx = int((1 - self.config.transfrmation_params.test_size) * len(df))\n",
    "        \n",
    "        train_df = df.iloc[:split_idx]\n",
    "        test_df = df.iloc[split_idx:]\n",
    "        \n",
    "        train_df.to_csv(self.config.train_file, index=False)\n",
    "        test_df.to_csv(self.config.test_file, index=False)\n",
    "    \n",
    "    def transformation_compose(self):\n",
    "        if self.config.dataset_val_status:\n",
    "            if not os.path.exists(self.config.train_file) and not os.path.exists(self.config.test_file):\n",
    "                df = self.loading_data()\n",
    "                df = self.handling_missing_values(df)\n",
    "                df = self.handling_datetime_features(df)\n",
    "                # df = self.log_transform(df)\n",
    "                df = self.feature_engineering(df)\n",
    "                df = self.handling_categorical_features(df)\n",
    "                self.train_test_split_and_save(df)\n",
    "            else:\n",
    "                print(\"The dataset has already been split and prepared.\")\n",
    "        else:\n",
    "            print(\"Dataset is not valid!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-24 14:18:06,905: INFO: main_utils: yaml file: config/config.yaml loaded successfully]\n",
      "[2025-06-24 14:18:06,913: INFO: main_utils: yaml file: params.yaml loaded successfully]\n",
      "[2025-06-24 14:18:06,917: INFO: main_utils: yaml file: schema.yaml loaded successfully]\n",
      "[2025-06-24 14:18:06,921: INFO: main_utils: created directory at: artifacts]\n",
      "[2025-06-24 14:18:06,924: INFO: main_utils: created directory at: artifacts/data_transformation]\n",
      "Features names: ['activity_month', 'account_id', 'brand_id', 'reg_date', 'ftd_date', 'qp_date', 'ben_login_id', 'tracker_id', 'player_reg_product', 'total_deposit', 'total_handle', 'total_ngr']\n",
      "Names of features of missing values: ['ftd_date', 'qp_date', 'total_handle']\n",
      "activity_month        0\n",
      "account_id            0\n",
      "brand_id              0\n",
      "reg_date              0\n",
      "ftd_date              0\n",
      "qp_date               0\n",
      "ben_login_id          0\n",
      "tracker_id            0\n",
      "player_reg_product    0\n",
      "total_deposit         0\n",
      "total_handle          0\n",
      "total_ngr             0\n",
      "dtype: int64\n",
      "Features names: ['activity_month', 'account_id', 'brand_id', 'reg_date', 'ftd_date', 'qp_date', 'ben_login_id', 'tracker_id', 'player_reg_product', 'total_deposit', 'total_handle', 'total_ngr']\n",
      "Names of datetime features: ['activity_month', 'reg_date', 'ftd_date', 'qp_date']\n",
      "Features names: ['brand_id', 'reg_date', 'ben_login_id', 'player_reg_product', 'total_deposit', 'total_handle', 'total_ngr', 'months_active', 'has_qp', 'days_ftd_to_qp', 'churn_next_month', 'early_churn']\n",
      "Names of categorical features: ['brand_id', 'ben_login_id', 'player_reg_product']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.transformation_compose()\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
